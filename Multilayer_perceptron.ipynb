{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyirobalazs/multilayer_perceptron/blob/main/Multilayer_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUIHhiaIITNs"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm \n",
        "from sklearn.datasets import fetch_openml\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9Mw611CIc5c"
      },
      "outputs": [],
      "source": [
        "class Data(object):\n",
        "\n",
        "  def __init__(self):\n",
        "      x_data, y_data = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "      x_data = (x_data/255).astype('float32')\n",
        "      y_data = to_categorical(y_data)\n",
        "      test_ratio = 0.14285714285714285714285714285714\n",
        "      self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x_data, y_data, test_size=test_ratio, random_state=42)\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ouv9shUD73g"
      },
      "outputs": [],
      "source": [
        "class Multilayer_Perceptron():\n",
        "    def __init__(self, hid_unit_num, max_epoch, learn_rate, finish_mode, decay_mode, decay_rate, energy_scale_maintenance, req_acc, cons_mode, cons_th, minibatch_size):\n",
        "        self.hid_unit_num = hid_unit_num\n",
        "        self.max_epoch = max_epoch\n",
        "        self.learn_rate = learn_rate\n",
        "        self.learning_time = 1\n",
        "        self.finish_mode = finish_mode\n",
        "        self.permanent = {} #just to transfer IN&OUT values between forward and backward prop.\n",
        "        self.decay_mode = decay_mode\n",
        "        self.decay_rate = decay_rate\n",
        "        self.energy_scale_maintenance = energy_scale_maintenance\n",
        "        self.trans_energy = 0\n",
        "        self.cons_energy = 0\n",
        "        self.perc_energy = 0\n",
        "        self.required_acc = req_acc\n",
        "        self.cons_mode = cons_mode\n",
        "        self.cons_treshold = cons_th\n",
        "        self.save = {} # for testing\n",
        "        self.minibatch_size = minibatch_size\n",
        "        \n",
        "        # save paramaters into a dictionary\n",
        "        self.parameters_initial, self.parameters_persistent, self.parameters_transient, self.parameters_history, self.permanent = self.initialization()\n",
        "\n",
        "\n",
        "    def __get_total_weights__(self):\n",
        "        '''\n",
        "        It gives back: total memory = initial - (transient + persistent)\n",
        "        '''\n",
        "        WT1_id = np.add(self.parameters_persistent['WT1'], self.parameters_transient['WT1'])\n",
        "        WT2_id = np.add(self.parameters_persistent['WT2'], self.parameters_transient['WT2'])\n",
        "        WT1_sum = np.subtract(self.parameters_initial['WT1'], WT1_id)\n",
        "        WT2_sum = np.subtract(self.parameters_initial['WT2'], WT2_id)\n",
        "        \n",
        "        total_weights  = {\n",
        "            'WT1' : WT1_sum,\n",
        "            'WT2' : WT2_sum\n",
        "        }\n",
        "\n",
        "        return total_weights\n",
        "    \n",
        "\n",
        "    def __set_total_weights__(self):\n",
        "        '''\n",
        "        1. Total memory = initial - (transient + persistent)\n",
        "        2. Refresh permanent memory\n",
        "        '''\n",
        "        \n",
        "        WT1_id = np.add(self.parameters_persistent['WT1'], self.parameters_transient['WT1'])\n",
        "        WT2_id = np.add(self.parameters_persistent['WT2'], self.parameters_transient['WT2'])\n",
        "        WT1_sum = np.subtract(self.parameters_initial['WT1'], WT1_id)\n",
        "        WT2_sum = np.subtract(self.parameters_initial['WT2'], WT2_id)\n",
        "        self.permanent['WT1'] = WT1_sum\n",
        "        self.permanent['WT2'] = WT2_sum\n",
        "\n",
        "\n",
        "    def sigmoid_function(self, x, derivate=False):\n",
        "        if derivate:\n",
        "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "        return 1/(1 + np.exp(-x))\n",
        "\n",
        "    def synaptic_decay(self):\n",
        "        '''\n",
        "        Exponential decay with decay rate. new_weights = old_weights*(1-decay_rate)^predictation_timestep. It only does if decay mode is True.\n",
        "        '''\n",
        "        if self.decay_mode:\n",
        "            self.parameters_transient['WT1'] *= np.power(np.exp(-self.decay_rate),1)\n",
        "            self.parameters_transient['WT2'] *= np.power(np.exp(-self.decay_rate),1)\n",
        "\n",
        "    def softmax_function(self, x, derivate=False):\n",
        "        expons = np.exp(x - x.max())\n",
        "        if derivate:\n",
        "            return expons / np.sum(expons, axis=0) * (1 - expons / np.sum(expons, axis=0))\n",
        "        return expons / np.sum(expons, axis=0)\n",
        "\n",
        "    def trans_energy_calc(self):\n",
        "        if self.cons_mode == \"no_cons\":\n",
        "            self.trans_energy += 0\n",
        "        else:\n",
        "            self.trans_energy += self.energy_scale_maintenance * ( np.sum( np.fabs(self.parameters_transient['WT1'])) + np.sum( np.fabs(self.parameters_transient['WT2'])) )\n",
        "\n",
        "    def min_energy_calc(self):\n",
        "        current_weights = self.__get_total_weights__()\n",
        "        difference_WT1 = current_weights['WT1'] - self.parameters_initial['WT1']\n",
        "        difference_WT2 = current_weights['WT2'] - self.parameters_initial['WT2']\n",
        "        min_energy = np.sum( np.fabs(difference_WT1)) + np.sum( np.fabs(difference_WT2))\n",
        "\n",
        "        return min_energy\n",
        "\n",
        "    def cons_energy_calc(self, weight_difference):\n",
        "        self.cons_energy += np.sum( np.fabs(weight_difference['WT1'])) + np.sum( np.fabs(weight_difference['WT2']))  \n",
        "\n",
        "    def perc_energy_calc(self, weight_change):\n",
        "        self.perc_energy += np.sum( np.fabs(weight_change['WT1'])) + np.sum( np.fabs(weight_change['WT2']))        \n",
        "\n",
        "\n",
        "    def consolidation(self):\n",
        "        \n",
        "        old_weights = copy.deepcopy(self.parameters_persistent)\n",
        "\n",
        "        if self.cons_mode == \"no_cons\":\n",
        "            self.parameters_persistent['WT1'] += self.parameters_transient['WT1']\n",
        "            self.parameters_persistent['WT2'] += self.parameters_transient['WT2']\n",
        "            self.parameters_transient['WT1'] = np.zeros((self.hidden_layer, self.input_layer))\n",
        "            self.parameters_transient['WT2'] = np.zeros((self.output_layer, self.hidden_layer))\n",
        "\n",
        "\n",
        "        elif self.cons_mode == \"lc_th_lc_cons\":\n",
        "          for dict_indexes, dict_params in self.parameters_transient.items():\n",
        "              for array_indexes, np_arrays in enumerate(dict_params):\n",
        "                  for val_indexes, values in enumerate(np_arrays):\n",
        "                    if np.fabs(values)>=self.cons_treshold: \n",
        "                      self.parameters_persistent[dict_indexes][array_indexes][val_indexes] += self.parameters_transient[dict_indexes][array_indexes][val_indexes]\n",
        "                      self.parameters_transient[dict_indexes][array_indexes][val_indexes] = 0.00\n",
        "        \n",
        "        elif self.cons_mode == \"lc_th_gl_cons\":\n",
        "          count_above_th = 0\n",
        "          for dict_indexes, dict_params in self.parameters_transient.items():\n",
        "              for array_indexes, np_arrays in enumerate(dict_params):\n",
        "                  for val_indexes, values in enumerate(np_arrays):\n",
        "                    if np.fabs(values)>=self.cons_treshold:\n",
        "                      count_above_th += 1\n",
        "\n",
        "          if count_above_th>0:\n",
        "              self.parameters_persistent['WT1'] += self.parameters_transient['WT1']\n",
        "              self.parameters_persistent['WT2'] += self.parameters_transient['WT2']\n",
        "              self.parameters_transient['WT1'] = np.zeros((self.hidden_layer, self.input_layer))\n",
        "              self.parameters_transient['WT2'] = np.zeros((self.output_layer, self.hidden_layer))\n",
        "\n",
        "        elif self.cons_mode == \"gl_th_gl_cons\":\n",
        "          count_above_th = 0\n",
        "          count_len_dict = 0\n",
        "          for dict_indexes, dict_params in self.parameters_transient.items():\n",
        "              for array_indexes, np_arrays in enumerate(dict_params):\n",
        "                  for val_indexes, values in enumerate(np_arrays):\n",
        "                    count_len_dict += 1                    \n",
        "                    if np.fabs(values)>=self.cons_treshold:\n",
        "                      count_above_th += 1\n",
        "\n",
        "          if count_above_th == count_len_dict:\n",
        "              self.parameters_persistent['WT1'] += self.parameters_transient['WT1']\n",
        "              self.parameters_persistent['WT2'] += self.parameters_transient['WT2']\n",
        "              self.parameters_transient['WT1'] = np.zeros((self.hidden_layer, self.input_layer))\n",
        "              self.parameters_transient['WT2'] = np.zeros((self.output_layer, self.hidden_layer))\n",
        "          \n",
        "        elif self.cons_mode_ != \"lc_th_lc_cons\" or self.cons_mode_ != \"lc_th_gl_cons\" or self.cons_mode_ != \"gl_th_gl_cons\" :\n",
        "            raise ValueError(\"Not existing consoldation mode\")\n",
        "        else:\n",
        "            raise ValueError(\"Value problem in the consolidation function\")\n",
        "\n",
        "        weight_difference = {\n",
        "            'WT1' : self.parameters_persistent['WT1'] - old_weights['WT1'],\n",
        "            'WT2' : self.parameters_persistent['WT2'] - old_weights['WT2']\n",
        "        }\n",
        "        self.cons_energy_calc(weight_difference)\n",
        "\n",
        "\n",
        "    def initialization(self):\n",
        "        '''\n",
        "        Set number of nodes in layers\n",
        "        '''\n",
        "\n",
        "        self.input_layer = 784\n",
        "        self.hidden_layer = self.hid_unit_num\n",
        "        self.output_layer = 10\n",
        "\n",
        "        parameters_initial = {\n",
        "            'WT1':np.random.randn(self.hidden_layer, self.input_layer) * np.sqrt(1. / self.hidden_layer),\n",
        "            'WT2':np.random.randn(self.output_layer, self.hidden_layer) * np.sqrt(1. / self.output_layer)\n",
        "        }\n",
        "\n",
        "        parameters_persistent = {\n",
        "            'WT1':np.zeros((self.hidden_layer, self.input_layer)),\n",
        "            'WT2':np.zeros((self.output_layer, self.hidden_layer))\n",
        "        }\n",
        "\n",
        "        parameters_transient = {\n",
        "            'WT1':np.zeros((self.hidden_layer, self.input_layer)),\n",
        "            'WT2':np.zeros((self.output_layer, self.hidden_layer))\n",
        "        }\n",
        "\n",
        "        permanent = {}\n",
        "\n",
        "        parameters_history = {\n",
        "            'time': [],\n",
        "            'epoch' : [],\n",
        "            'accuracy' : [],\n",
        "            'min_energy' : [],\n",
        "            'con_energy' : [],\n",
        "            'trans_energy': [],\n",
        "            'perceptron_energy': [],\n",
        "            'total_energy' : []\n",
        "        }\n",
        "\n",
        "        return parameters_initial, parameters_persistent, parameters_transient, parameters_history, permanent\n",
        "\n",
        "    def forward_propagation(self, x_train):\n",
        "        '''\n",
        "        This is the forward propagation algorithm, for calculating the updates of \n",
        "        the neural network's parameters in the forward direction. It uses sigmoid\n",
        "        funcion at the hidden layer and softmax at the output layer.\n",
        "        '''\n",
        "        \n",
        "        self.__set_total_weights__()\n",
        "        parameters_total = self.permanent\n",
        "\n",
        "        # input layer activations becomes sample\n",
        "        parameters_total['OUT_0'] = x_train\n",
        "\n",
        "        # input layer to hidden layer 1\n",
        "        parameters_total['IN_1'] = np.dot(parameters_total['WT1'], parameters_total['OUT_0'])\n",
        "        parameters_total['OUT_1'] = self.sigmoid_function(parameters_total['IN_1'])\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        parameters_total['IN_2'] = np.dot(parameters_total['WT2'], parameters_total['OUT_1'])\n",
        "        parameters_total['OUT_2'] = self.softmax_function(parameters_total['IN_2'])\n",
        "\n",
        "        return parameters_total['OUT_2']\n",
        "\n",
        "    def backpropagation(self, y_train, output):\n",
        "        '''\n",
        "        This is the backpropagation algorithm, for calculating the updates of \n",
        "        the neural network's parameters in the backward direction.\n",
        "        '''\n",
        "\n",
        "        parameters_total = self.permanent\n",
        "        weight_change = {}\n",
        "\n",
        "        # Calculate weight changes between the output and the hidden layer\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.softmax_function(parameters_total['IN_2'], derivate=True)\n",
        "        weight_change['WT2'] = np.outer(error, parameters_total['OUT_1'])\n",
        "\n",
        "        # Calculate weight changes between the hidden layer and the input layer\n",
        "        error = np.dot(parameters_total['WT2'].T, error) * self.sigmoid_function(parameters_total['IN_1'], derivate=True)\n",
        "        weight_change['WT1'] = np.outer(error, parameters_total['OUT_0'])\n",
        "\n",
        "        return weight_change\n",
        "\n",
        "\n",
        "    def update_network(self, w_change):\n",
        "        '''\n",
        "        Update network parameters according to Stochastic Gradient Descent\n",
        "        '''\n",
        "\n",
        "        for ind, val in w_change.items():\n",
        "            self.parameters_transient[ind] += self.learn_rate * val\n",
        "\n",
        "       \n",
        "    def compute_accuracy(self, x_test, y_test):\n",
        "        '''\n",
        "        This function performs a forward pass of x, then checks if the indices \n",
        "        of the output's highest value match the indices in the label y. After \n",
        "        that, it adds together all of the prediction and calculates the accuracy.\n",
        "        '''\n",
        "\n",
        "        pred = []\n",
        "\n",
        "        for x__test_data, y__test_data in zip(x_test, y_test):\n",
        "            output = self.forward_propagation(x__test_data)\n",
        "            ind_pred = np.argmax(output)\n",
        "            pred.append(ind_pred == np.argmax(y__test_data))\n",
        "        \n",
        "        return np.mean(pred)\n",
        "\n",
        "\n",
        "    def train(self, x_train, y_train, x_test, y_test):\n",
        "        '''\n",
        "        This functions run through the epochs and call the forward, backward propagation\n",
        "        and the weight update functions. After each epocs it calculates an accuracy.\n",
        "        '''\n",
        "\n",
        "        start_time = time.time()\n",
        "        for iter in range(self.max_epoch):\n",
        "            print()\n",
        "            print(f\"Processing epoch: {iter+1}/{self.max_epoch} --> \", end = ' ')\n",
        "            for ind_train_pic,ind_train_label in tqdm(zip(x_train, y_train), total = x_train.shape[0]):\n",
        "                self.learning_time +=1\n",
        "                output = self.forward_propagation(ind_train_pic)\n",
        "                w_change = self.backpropagation(ind_train_label, output)\n",
        "                self.update_network(w_change)\n",
        "                self.perc_energy_calc(w_change)\n",
        "                self.trans_energy_calc()\n",
        "\n",
        "                if(((self.learning_time+1)%self.minibatch_size)==0):\n",
        "                    min_energy = self.min_energy_calc()\n",
        "                    total_energy = self.trans_energy + self.cons_energy + self.perc_energy\n",
        "                    batch_accuracy = self.compute_accuracy(x_test, y_test)\n",
        "                    self.synaptic_decay()\n",
        "                    self.consolidation()\n",
        "\n",
        "                    self.parameters_history['time'].append(self.learning_time)\n",
        "                    self.parameters_history['epoch'].append(iter)\n",
        "                    self.parameters_history['accuracy'].append(batch_accuracy)\n",
        "                    self.parameters_history['min_energy'].append(min_energy)\n",
        "                    self.parameters_history['con_energy'].append(self.cons_energy)\n",
        "                    self.parameters_history['trans_energy'].append(self.trans_energy)\n",
        "                    self.parameters_history['total_energy'].append(total_energy)\n",
        "                    self.parameters_history['perceptron_energy'].append(self.perc_energy)\n",
        "\n",
        "                    if batch_accuracy >= self.required_acc and self.finish_mode == \"success_rate\": \n",
        "                        return self.parameters_history\n",
        "                    elif batch_accuracy < self.required_acc and self.finish_mode == \"till_the_end\":\n",
        "                        pass                    \n",
        "\n",
        "                else:\n",
        "                    self.synaptic_decay()\n",
        "                    self.trans_energy_calc()\n",
        "                    pass\n",
        "\n",
        "            accuracy = self.compute_accuracy(x_test, y_test)\n",
        "            print(\"\\033[94m Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}% \\033[0m\".format(\n",
        "                iter+1, time.time() - start_time, accuracy * 100\n",
        "            ))\n",
        "\n",
        "            # if it learned the data it interrupts the run.\n",
        "            #if accuracy >= self.required_acc and self.finish_mode == \"success_rate\": \n",
        "            #    return iter, self.learning_time\n",
        "            #elif accuracy < self.required_acc and self.finish_mode == \"till_the_end\":\n",
        "            #    pass\n",
        "            \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwIvz6mrImJz",
        "outputId": "67ef1228-f5fa-449a-f62d-5591dd8ad71a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m Data initialization in progress... \u001b[0m\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "MAX_EPOCH = 2 #normaly =30\n",
        "HIDDEN_LAYER_UNITT_NUM = 100 #normaly = 100\n",
        "LEARNING_RATE = 0.1 #normaly = 0.1\n",
        "FINISH_MODE =  \"till_the_end\" # 1.till_the_end 2. success_rate\n",
        "DECAY_MODE = True # True if there is a decay, and False if there is no decay\n",
        "DECAY_RATE = 0.000001 #normaly = 0.001\n",
        "ENERGY_SCALE_MAINTENANCE = 0.001\n",
        "REQUIRED_ACCURACY = 100.0\n",
        "CONSOLIDATION_MODE = \"lc_th_lc_cons\" # 1. lc_th_lc_cons 2. lc_th_gl_cons 3. gl_th_gl_cons 4. no_cons\n",
        "CONSOLIDATION_TRESHOLD = 0.01 #normaly =0.01\n",
        "MINIBATCH_SIZE = 10000\n",
        "\n",
        "print(\"\\033[1m Data initialization in progress... \\033[0m\")\n",
        "dat = Data()\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#learning_rate_array = [0.001, 0.01, 0.1, 0.5]\n",
        "learning_rate_array = [0.01]\n",
        "histories = []\n",
        "for LEARNING_RATE in learning_rate_array:\n",
        "    mlp = Multilayer_Perceptron(HIDDEN_LAYER_UNITT_NUM, \n",
        "                                MAX_EPOCH, \n",
        "                                LEARNING_RATE, \n",
        "                                FINISH_MODE, \n",
        "                                DECAY_MODE, \n",
        "                                DECAY_RATE, \n",
        "                                ENERGY_SCALE_MAINTENANCE, \n",
        "                                REQUIRED_ACCURACY, \n",
        "                                CONSOLIDATION_MODE, \n",
        "                                CONSOLIDATION_TRESHOLD,\n",
        "                                MINIBATCH_SIZE\n",
        "                                )\n",
        " \n",
        "    print(\"\\033[1m Perceptron initialization done ==> \" + f\"Max epoch: {MAX_EPOCH} | Number of hidden units: {HIDDEN_LAYER_UNITT_NUM} | Learning rate: {LEARNING_RATE} | Consolidation mode: {CONSOLIDATION_MODE} | Stop accuracy: {REQUIRED_ACCURACY}  \\033[0m\")\n",
        "    mlp.train(dat.x_train, dat.y_train, dat.x_test, dat.y_test)\n",
        "    histories.append(mlp.parameters_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-RORuOuPCqK",
        "outputId": "f5f35d64-9ba7-44fc-88f9-559d4bd5faba"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m Perceptron initialization done ==> Max epoch: 2 | Number of hidden units: 100 | Learning rate: 0.01 | Consolidation mode: lc_th_lc_cons | Stop accuracy: 100.0  \u001b[0m\n",
            "\n",
            "Processing epoch: 1/2 -->  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60000/60000 [03:04<00:00, 324.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m Epoch: 1, Time Spent: 189.21s, Accuracy: 78.11% \u001b[0m\n",
            "\n",
            "Processing epoch: 2/2 -->  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60000/60000 [03:01<00:00, 330.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m Epoch: 2, Time Spent: 374.97s, Accuracy: 80.63% \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ind, val in enumerate(histories):\n",
        "  line_name = 'n=' + str(learning_rate_array[ind])\n",
        "  y = np.multiply(histories[ind]['total_energy'],0.01)\n",
        "  plt.plot(histories[ind]['accuracy'], y, label = line_name)\n",
        "\n",
        "plt.ylim((10000, 1000000))\n",
        "plt.xlim((0.8, 0.825))\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6OW-HoyQlPH-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "6e365e9a-92a6-4974-a438-225c8e1b02f5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Multilayer perceptron",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORNvm5qLzMlIqpUwmEdGHb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}